# Example LLM Service Configuration
# Copy this file to config/llm_service.yaml and customize for your setup

llm_service:
  # Default provider to use when none specified
  default_provider: "openai"

  # Fallback chain - providers to try in order if primary fails
  fallback_chain: ["openai", "anthropic", "ollama"]

  # Global settings
  request_timeout: 60        # seconds
  max_retries: 3            # maximum retry attempts
  retry_delay: 1.0          # base retry delay in seconds

  # Provider configurations
  providers:
    # OpenAI Configuration
    openai:
      enabled: true
      api_key: "${OPENAI_API_KEY}"  # Set via environment variable
      base_url: "https://api.openai.com/v1"

      # Available models
      models:
        - gpt-3.5-turbo
        - gpt-3.5-turbo-16k
        - gpt-4
        - gpt-4-32k
        - gpt-4-turbo
        - gpt-4-turbo-preview

      # Rate limiting (optional)
      rate_limits:
        requests_per_minute: 60
        tokens_per_minute: 40000

      # Provider-specific settings
      timeout: 60
      max_retries: 3

    # Anthropic Claude Configuration
    anthropic:
      enabled: true
      api_key: "${ANTHROPIC_API_KEY}"  # Set via environment variable

      # Available models
      models:
        - claude-3-haiku-20240307
        - claude-3-sonnet-20240229
        - claude-3-opus-20240229
        - claude-3-5-sonnet-20240620

      # Provider-specific settings
      timeout: 60
      max_retries: 3

    # Ollama Local Models Configuration
    ollama:
      enabled: true
      base_url: "http://localhost:11434"  # Default Ollama endpoint

      # Available local models (pull with: ollama pull <model>)
      models:
        - llama2              # ollama pull llama2
        - llama2:13b          # ollama pull llama2:13b
        - llama2:70b          # ollama pull llama2:70b
        - llama3              # ollama pull llama3
        - llama3:70b          # ollama pull llama3:70b
        - codellama           # ollama pull codellama
        - codellama:13b       # ollama pull codellama:13b
        - mistral             # ollama pull mistral
        - mistral:7b-instruct # ollama pull mistral:7b-instruct
        - gemma               # ollama pull gemma
        - gemma:7b            # ollama pull gemma:7b
        - phi3                # ollama pull phi3
        - qwen                # ollama pull qwen

      # Provider-specific settings
      timeout: 120            # Longer timeout for local models
      max_retries: 2

    # Azure OpenAI Configuration (disabled by default)
    azure_openai:
      enabled: false
      api_key: "${AZURE_OPENAI_API_KEY}"
      base_url: "${AZURE_OPENAI_ENDPOINT}"
      api_version: "2024-02-15-preview"

      # Azure-specific model deployments
      models:
        - gpt-35-turbo        # Your deployment name
        - gpt-4               # Your deployment name

      timeout: 60
      max_retries: 3

    # Google AI Configuration (disabled by default)
    google:
      enabled: false
      api_key: "${GOOGLE_AI_API_KEY}"

      models:
        - gemini-pro
        - gemini-pro-vision

      timeout: 60
      max_retries: 3

    # Hugging Face Configuration (disabled by default)
    huggingface:
      enabled: false
      api_key: "${HUGGINGFACE_API_KEY}"
      base_url: "https://api-inference.huggingface.co/models"

      models:
        - microsoft/DialoGPT-large
        - EleutherAI/gpt-j-6B
        - bigscience/bloom

      timeout: 120
      max_retries: 2

# Environment Variables Required:
#
# For OpenAI:
# export OPENAI_API_KEY="sk-..."
#
# For Anthropic:
# export ANTHROPIC_API_KEY="sk-ant-..."
#
# For Azure OpenAI:
# export AZURE_OPENAI_API_KEY="..."
# export AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
#
# For Google AI:
# export GOOGLE_AI_API_KEY="..."
#
# For Hugging Face:
# export HUGGINGFACE_API_KEY="hf_..."

# Quick Setup Commands:
#
# 1. Install Ollama:
#    curl -fsSL https://ollama.ai/install.sh | sh
#
# 2. Pull some local models:
#    ollama pull llama2
#    ollama pull codellama
#    ollama pull mistral
#
# 3. Start Ollama server:
#    ollama serve
#
# 4. Test local models:
#    ollama run llama2 "Hello, how are you?"
